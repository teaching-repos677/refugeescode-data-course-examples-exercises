{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals & Hypothesis Testing\n",
    "\n",
    "**Module: Descriptive & Inferential Statistics**\n",
    "\n",
    "## Learning Objectives\n",
    "- Construct and interpret confidence intervals\n",
    "- Set up and conduct hypothesis tests\n",
    "- Interpret p-values and make decisions\n",
    "- Calculate and interpret effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Refresher\n",
    "\n",
    "### Confidence Intervals\n",
    "A range of values likely to contain the true population parameter.\n",
    "\n",
    "**For a mean (when σ unknown):**\n",
    "$$\\bar{x} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "- 95% CI: We're 95% confident the true mean lies in this interval\n",
    "- Wider CI = more uncertainty, narrower = more precision\n",
    "\n",
    "### Hypothesis Testing\n",
    "| Term | Meaning |\n",
    "|------|--------|\n",
    "| **H₀ (null)** | No effect/difference (status quo) |\n",
    "| **H₁ (alternative)** | There is an effect/difference |\n",
    "| **p-value** | Probability of seeing results this extreme if H₀ is true |\n",
    "| **α (alpha)** | Significance level, typically 0.05 |\n",
    "| **Decision** | If p < α, reject H₀ |\n",
    "\n",
    "### Common Tests\n",
    "| Scenario | Test |\n",
    "|----------|------|\n",
    "| One sample mean vs. known value | One-sample t-test |\n",
    "| Two independent group means | Two-sample t-test |\n",
    "| Same group, before/after | Paired t-test |\n",
    "| Proportions | z-test for proportions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Working Example: Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of customer satisfaction scores\n",
    "satisfaction = np.array([4.2, 3.8, 4.5, 4.1, 3.9, 4.3, 4.0, 3.7, 4.4, 4.2,\n",
    "                         3.6, 4.1, 4.3, 3.9, 4.0, 4.2, 3.8, 4.1, 4.4, 3.9])\n",
    "\n",
    "n = len(satisfaction)\n",
    "mean = satisfaction.mean()\n",
    "std = satisfaction.std(ddof=1)  # Sample std dev\n",
    "se = std / np.sqrt(n)  # Standard error\n",
    "\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Sample mean: {mean:.3f}\")\n",
    "print(f\"Sample std: {std:.3f}\")\n",
    "print(f\"Standard error: {se:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% Confidence Interval using t-distribution\n",
    "confidence = 0.95\n",
    "alpha = 1 - confidence\n",
    "df = n - 1  # degrees of freedom\n",
    "\n",
    "# t critical value for 95% CI\n",
    "t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "print(f\"t critical value: {t_crit:.3f}\")\n",
    "\n",
    "# Margin of error\n",
    "margin = t_crit * se\n",
    "\n",
    "# CI bounds\n",
    "ci_lower = mean - margin\n",
    "ci_upper = mean + margin\n",
    "\n",
    "print(f\"\\n95% Confidence Interval: ({ci_lower:.3f}, {ci_upper:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier way using scipy.stats\n",
    "ci = stats.t.interval(confidence=0.95, df=n-1, loc=mean, scale=se)\n",
    "print(f\"95% CI (scipy): ({ci[0]:.3f}, {ci[1]:.3f})\")\n",
    "\n",
    "# Or using sem (standard error of mean)\n",
    "sem = stats.sem(satisfaction)\n",
    "ci_alt = stats.t.interval(0.95, df=n-1, loc=mean, scale=sem)\n",
    "print(f\"95% CI (using sem): ({ci_alt[0]:.3f}, {ci_alt[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Working Example: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test\n",
    "# H₀: μ = 4.0 (target satisfaction is 4.0)\n",
    "# H₁: μ ≠ 4.0\n",
    "\n",
    "target = 4.0\n",
    "t_stat, p_value = stats.ttest_1samp(satisfaction, target)\n",
    "\n",
    "print(f\"H₀: Population mean = {target}\")\n",
    "print(f\"H₁: Population mean ≠ {target}\")\n",
    "print(f\"\\nt-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"\\nConclusion at α=0.05: {'Reject H₀' if p_value < 0.05 else 'Fail to reject H₀'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "# Compare satisfaction between two store locations\n",
    "\n",
    "store_a = np.array([4.2, 4.5, 3.9, 4.1, 4.3, 4.0, 4.4, 3.8, 4.2, 4.1,\n",
    "                    4.3, 3.9, 4.2, 4.0, 4.1])\n",
    "store_b = np.array([3.8, 3.5, 4.0, 3.7, 3.9, 3.6, 3.8, 4.1, 3.7, 3.5,\n",
    "                    3.9, 3.6, 3.8, 3.7, 4.0])\n",
    "\n",
    "print(f\"Store A: mean = {store_a.mean():.3f}, n = {len(store_a)}\")\n",
    "print(f\"Store B: mean = {store_b.mean():.3f}, n = {len(store_b)}\")\n",
    "\n",
    "# Two-sample independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(store_a, store_b)\n",
    "\n",
    "print(f\"\\nt-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"\\nConclusion: {'Significant difference' if p_value < 0.05 else 'No significant difference'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test (before/after)\n",
    "# Productivity scores before and after training\n",
    "\n",
    "before = np.array([72, 65, 80, 68, 75, 70, 78, 62, 74, 69])\n",
    "after = np.array([78, 70, 82, 75, 80, 74, 81, 68, 79, 75])\n",
    "\n",
    "print(f\"Before: mean = {before.mean():.1f}\")\n",
    "print(f\"After: mean = {after.mean():.1f}\")\n",
    "print(f\"Average improvement: {(after - before).mean():.1f}\")\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(after, before)\n",
    "\n",
    "print(f\"\\nt-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"\\nConclusion: {'Training had significant effect' if p_value < 0.05 else 'No significant effect'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size: Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d for two independent groups.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(ddof=1), group2.var(ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    \n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "d = cohens_d(store_a, store_b)\n",
    "print(f\"Cohen's d: {d:.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  |d| < 0.2: negligible\")\n",
    "print(f\"  |d| ~ 0.2: small\")\n",
    "print(f\"  |d| ~ 0.5: medium\")\n",
    "print(f\"  |d| ~ 0.8+: large\")\n",
    "print(f\"\\nThis effect is: {'large' if abs(d) >= 0.8 else 'medium' if abs(d) >= 0.5 else 'small' if abs(d) >= 0.2 else 'negligible'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer wait times (in minutes) at a service center\n",
    "wait_times = np.array([5.2, 7.1, 4.8, 6.3, 8.2, 5.5, 6.8, 4.2, 7.5, 6.1,\n",
    "                       5.8, 7.3, 6.5, 4.9, 5.7, 8.0, 6.2, 5.4, 7.0, 6.6,\n",
    "                       5.3, 6.9, 7.4, 5.1, 6.4])\n",
    "\n",
    "# TODO: Calculate the 95% confidence interval for mean wait time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate a 90% confidence interval. Is it wider or narrower than 95%?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The target is to keep average wait time under 6 minutes.\n",
    "# Based on the 95% CI, can you confidently say you're meeting this target?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: One-Sample Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A manufacturer claims their batteries last 500 hours on average.\n",
    "# You test 30 batteries and get these results:\n",
    "\n",
    "battery_life = np.array([485, 512, 478, 495, 520, 488, 502, 475, 498, 510,\n",
    "                         492, 505, 480, 515, 490, 508, 483, 500, 495, 518,\n",
    "                         487, 503, 477, 512, 493, 506, 481, 499, 515, 489])\n",
    "\n",
    "# TODO: Test the manufacturer's claim at α = 0.05\n",
    "# H₀: μ = 500\n",
    "# H₁: μ ≠ 500 (two-tailed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now test if batteries last LESS than 500 hours (one-tailed)\n",
    "# H₀: μ ≥ 500\n",
    "# H₁: μ < 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Two-Sample Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two different training programs for sales team\n",
    "# Performance scores after completing each program\n",
    "\n",
    "program_a = np.array([82, 78, 85, 80, 83, 79, 86, 81, 77, 84,\n",
    "                      80, 82, 78, 85, 81, 83, 79, 86, 82, 80])\n",
    "\n",
    "program_b = np.array([88, 85, 90, 87, 92, 86, 89, 84, 91, 88,\n",
    "                      87, 90, 85, 93, 89, 86, 91, 88, 87, 90])\n",
    "\n",
    "# TODO: Test if there's a significant difference between programs at α = 0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Cohen's d effect size. How meaningful is the difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate 95% CI for the difference in means\n",
    "# Hint: You can use stats.ttest_ind with equal_var=True to get a combined view,\n",
    "# or calculate: (mean_diff) ± t_crit * SE_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Paired Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website page load times (seconds) before and after optimization\n",
    "# Same 15 pages measured both times\n",
    "\n",
    "pages = pd.DataFrame({\n",
    "    'page_id': range(1, 16),\n",
    "    'before': [3.2, 4.1, 2.8, 5.2, 3.8, 4.5, 3.1, 4.8, 3.5, 4.2,\n",
    "               3.9, 2.9, 4.6, 3.3, 4.0],\n",
    "    'after': [2.8, 3.5, 2.5, 4.1, 3.2, 3.8, 2.7, 4.0, 3.0, 3.5,\n",
    "              3.3, 2.5, 3.9, 2.9, 3.4]\n",
    "})\n",
    "\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the improvement for each page\n",
    "# What's the mean improvement?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform a paired t-test to see if the optimization significantly reduced load times\n",
    "# H₀: μ_diff = 0 (no change)\n",
    "# H₁: μ_diff > 0 (load times decreased, meaning before > after)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate 95% CI for the mean improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Proportion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email campaign results\n",
    "# Historical click-through rate: 2.5%\n",
    "# New campaign: 58 clicks out of 2000 emails sent\n",
    "\n",
    "clicks = 58\n",
    "total = 2000\n",
    "historical_rate = 0.025\n",
    "\n",
    "# TODO: Calculate the sample proportion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test if the new campaign has a different CTR than historical\n",
    "# Use a z-test for proportion\n",
    "# z = (p_hat - p0) / sqrt(p0 * (1-p0) / n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate 95% CI for the true click-through rate\n",
    "# CI = p_hat ± z_crit * sqrt(p_hat * (1 - p_hat) / n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solutions\n",
    "\n",
    "n = len(wait_times)\n",
    "mean = wait_times.mean()\n",
    "sem = stats.sem(wait_times)\n",
    "\n",
    "# 95% CI\n",
    "ci_95 = stats.t.interval(0.95, df=n-1, loc=mean, scale=sem)\n",
    "print(f\"95% CI: ({ci_95[0]:.3f}, {ci_95[1]:.3f})\")\n",
    "\n",
    "# 90% CI\n",
    "ci_90 = stats.t.interval(0.90, df=n-1, loc=mean, scale=sem)\n",
    "print(f\"90% CI: ({ci_90[0]:.3f}, {ci_90[1]:.3f})\")\n",
    "print(\"90% CI is narrower (less confidence = narrower interval)\")\n",
    "\n",
    "# Target assessment\n",
    "print(f\"\\nSample mean: {mean:.3f}\")\n",
    "print(f\"95% CI lower bound: {ci_95[0]:.3f}\")\n",
    "if ci_95[1] < 6:\n",
    "    print(\"Yes, can confidently say average wait time is under 6 minutes\")\n",
    "else:\n",
    "    print(\"No, cannot confidently say average is under 6 minutes (CI includes values ≥ 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solutions\n",
    "\n",
    "# Two-tailed test\n",
    "t_stat, p_value = stats.ttest_1samp(battery_life, 500)\n",
    "print(f\"Two-tailed test:\")\n",
    "print(f\"Sample mean: {battery_life.mean():.2f}\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusion: {'Reject H₀' if p_value < 0.05 else 'Fail to reject H₀'}\")\n",
    "\n",
    "# One-tailed test (less than)\n",
    "p_one_tail = p_value / 2 if t_stat < 0 else 1 - p_value / 2\n",
    "print(f\"\\nOne-tailed test (μ < 500):\")\n",
    "print(f\"p-value: {p_one_tail:.4f}\")\n",
    "print(f\"Conclusion: {'Evidence batteries last less than 500 hours' if p_one_tail < 0.05 else 'No evidence batteries last less than 500 hours'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solutions\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(program_a, program_b)\n",
    "print(f\"Program A mean: {program_a.mean():.2f}\")\n",
    "print(f\"Program B mean: {program_b.mean():.2f}\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.6f}\")\n",
    "print(f\"Conclusion: {'Significant difference' if p_value < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "# Cohen's d\n",
    "d = cohens_d(program_b, program_a)\n",
    "print(f\"\\nCohen's d: {d:.3f} (large effect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI for difference in means\n",
    "mean_diff = program_b.mean() - program_a.mean()\n",
    "n1, n2 = len(program_a), len(program_b)\n",
    "se_diff = np.sqrt(program_a.var(ddof=1)/n1 + program_b.var(ddof=1)/n2)\n",
    "df = n1 + n2 - 2\n",
    "t_crit = stats.t.ppf(0.975, df)\n",
    "\n",
    "ci_diff = (mean_diff - t_crit * se_diff, mean_diff + t_crit * se_diff)\n",
    "print(f\"95% CI for difference: ({ci_diff[0]:.2f}, {ci_diff[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 Solutions\n",
    "\n",
    "pages['improvement'] = pages['before'] - pages['after']\n",
    "print(f\"Mean improvement: {pages['improvement'].mean():.3f} seconds\")\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(pages['before'], pages['after'])\n",
    "# For one-tailed (before > after), divide p by 2\n",
    "p_one_tail = p_value / 2\n",
    "\n",
    "print(f\"\\nt-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value (one-tailed): {p_one_tail:.6f}\")\n",
    "print(f\"Conclusion: {'Optimization significantly reduced load times' if p_one_tail < 0.05 else 'No significant reduction'}\")\n",
    "\n",
    "# CI for improvement\n",
    "ci_imp = stats.t.interval(0.95, df=len(pages)-1, \n",
    "                          loc=pages['improvement'].mean(), \n",
    "                          scale=stats.sem(pages['improvement']))\n",
    "print(f\"\\n95% CI for mean improvement: ({ci_imp[0]:.3f}, {ci_imp[1]:.3f}) seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 Solutions\n",
    "\n",
    "# Sample proportion\n",
    "p_hat = clicks / total\n",
    "print(f\"Sample proportion: {p_hat:.4f} ({p_hat*100:.2f}%)\")\n",
    "\n",
    "# Z-test for proportion\n",
    "z_stat = (p_hat - historical_rate) / np.sqrt(historical_rate * (1 - historical_rate) / total)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))  # Two-tailed\n",
    "\n",
    "print(f\"\\nZ-statistic: {z_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusion: {'CTR is different from historical' if p_value < 0.05 else 'No significant difference from historical'}\")\n",
    "\n",
    "# CI for proportion\n",
    "z_crit = stats.norm.ppf(0.975)\n",
    "margin = z_crit * np.sqrt(p_hat * (1 - p_hat) / total)\n",
    "ci_prop = (p_hat - margin, p_hat + margin)\n",
    "print(f\"\\n95% CI for CTR: ({ci_prop[0]*100:.2f}%, {ci_prop[1]*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MFR    \n  Teaching",
   "language": "python",
   "name": "mfr-teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
